# WebcrawlerRepo
Webcrawler and BeautifulSoup Repo
WebcrawlerRepo is a lightweight, production-ready Flask application that:

âœ… Crawls an entire website, including all subpages

âœ… Cleans and extracts readable text from HTML pages

âœ… Uploads clean .txt files to Google Drive

âœ… Provides a simple API endpoint to trigger the whole workflow

Built to help power Retrieval-Augmented Generation (RAG) pipelines, AI knowledge bases, and document ingestion processes with minimal setup.

ğŸš€ Features
Website crawling using wget

Text extraction with BeautifulSoup (removes menus, scripts, footers, etc.)

Automatic upload of clean data to Google Drive

Cloud-deployable (designed for platforms like Render.com)

Simple API interface â€” send a POST request, get results

ğŸ›  Project Structure
plaintext
Copy
Edit
/
â”œâ”€â”€ app.py                  # Main Flask application
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ crawler.py          # Crawls websites using wget
â”‚   â”œâ”€â”€ cleaner.py          # Cleans and extracts readable text
â”‚   â””â”€â”€ uploader.py         # Uploads files to Google Drive
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ .gitignore              # Python cache + auth file exclusions
âš™ï¸ Setup Instructions
1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/YOUR_USERNAME/WebcrawlerRepo.git
cd WebcrawlerRepo
2. Install Dependencies
Create a virtual environment (recommended):

bash
Copy
Edit
python3 -m venv venv
source venv/bin/activate    # On Windows: venv\Scripts\activate
Install required Python libraries:

bash
Copy
Edit
pip install -r requirements.txt
3. Configure Google Drive Authentication
This app uses PyDrive to upload files.
You'll need to authenticate the first time manually:

bash
Copy
Edit
python
>>> from pydrive.auth import GoogleAuth
>>> gauth = GoogleAuth()
>>> gauth.LocalWebserverAuth()
Follow the browser popup and login process.
It will save your credentials locally.

4. Run Locally
bash
Copy
Edit
python app.py
App will start at http://localhost:8000.

ğŸŒ API Usage
ğŸ“ POST /crawl
Endpoint:
/crawl

Body Parameters:

json
Copy
Edit
{
  "url": "https://example.com"
}
What happens:

Crawls the full website

Extracts clean text

Uploads .txt files to Google Drive

Responds with a success or error message

ğŸš€ Deploying to Render
You can easily deploy this app to Render.com:

Create a new Web Service.

Connect your GitHub repo.

Set:

Build Command: pip install -r requirements.txt

Start Command: python app.py

Set environment to Python 3.

Hit Deploy!

âš¡ Improvements (Future Roadmap)
Auto-create a new Google Drive folder for each website crawl

Compress extracted text into a .zip archive for easy download

Support scheduled crawling (e.g., daily/weekly updates)

Add multi-threaded crawling for large websites

Optional S3 / Azure blob storage upload instead of Drive

ğŸ¤ Contributions
Pull requests and contributions are welcome!
Please open an issue first to discuss major changes.

ğŸ“œ License
This project is licensed under the MIT License.
